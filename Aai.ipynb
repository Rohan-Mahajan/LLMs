from google.colab import userdata
import pandas as pd
import numpy as np
import logging
import random
import threading
import sys
import time
from typing import TypedDict, List

# Import LangGraph and related components
from langgraph.graph import END, StateGraph
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_groq import ChatGroq

# Setup logging for debugging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --------------------------------------------------------------------
# Data Loading and Document Preparation
# --------------------------------------------------------------------
groq_api = userdata.get("groq_api_key")
df = pd.read_csv("/content/defects.csv")             # Contains defects and solutions
test_cases_df = pd.read_csv("/content/test_cases.csv")  # Contains test cases with columns: 
                                                      # "Module", "Type" (positive/negative),
                                                      # "Test Scenario", "Test Steps",
                                                      # "Pre Requisites", "Expected Results", 
                                                      # "Pass/Fail Criteria"

docs = []
for _, row in df.iterrows():
    if pd.notna(row["Description"]) and pd.notna(row["Solution"]):
        docs.append(Document(
            page_content=row["Description"],
            metadata={
                "solution": row["Solution"],
                "module": row["Module"]
            }
        ))

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_documents(docs, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 1})

# --------------------------------------------------------------------
# Define Agent State and LLM Initialization
# --------------------------------------------------------------------
class AgentState(TypedDict):
    input: str
    context: List[Document]
    response: str

llm = ChatGroq(
    groq_api_key=groq_api,
    temperature=0.3,
    model_name="gemma2-9b-it",
)

# --------------------------------------------------------------------
# Helper: Classify Test Case (Positive/Negative) with Enhanced Keywords
# --------------------------------------------------------------------
def classify_test_case(tc_text: str) -> str:
    negative_keywords = [
        "fail", "error", "misconfigured", "incorrect", "doesn't work",
        "not work", "negative", "invalid", "wrong", "missing", "unexpected",
        "should not", "incorrectly", "failure", "reject", "malformed",
        "timeout", "invalid input", "edge case", "out of bounds"
    ]
    text_lower = tc_text.lower()
    return "negative" if any(kw in text_lower for kw in negative_keywords) else "positive"

# --------------------------------------------------------------------
# Workflow Node: Validate or Generate Test Cases (Using CSV if available)
# --------------------------------------------------------------------
def validate_or_generate_test_cases(state: AgentState):
    try:
        # Check if context exists; otherwise, error out.
        if not state["context"]:
            return {"response": "**Error**: The defect could not be found in the database."}
        context = state["context"][0]
        error_message = state["input"]
        solution = context.metadata["solution"]
        module = context.metadata["module"]

        # ----------------------------------------------------------------
        # Step 1: Fetch test cases from CSV for the given module.
        # ----------------------------------------------------------------
        # Filter CSV test cases for the defect's module
        module_test_cases = test_cases_df[test_cases_df["Module"] == module]

        # Separate into positive and negative based on the "Type" column.
        csv_positive = module_test_cases[module_test_cases["Type"].str.lower() == "positive"]
        csv_negative = module_test_cases[module_test_cases["Type"].str.lower() == "negative"]

        # Initialize lists for final test cases.
        final_pos_cases = []
        final_neg_cases = []

        # Helper to format a test case from CSV row.
        def format_csv_test_case(row):
            return (
                f"**Test Scenario**: {row['Test Scenario']}\n"
                f"**Test Steps**: {row['Test Steps']}\n"
                f"**Pre Requisites**: {row['Pre Requisites']}\n"
                f"**Expected Results**: {row['Expected Results']}\n"
                f"**Pass/Fail Criteria**: {row['Pass/Fail Criteria']}"
            )

        # Use up to 2 existing positive test cases.
        for _, row in csv_positive.head(2).iterrows():
            final_pos_cases.append(format_csv_test_case(row))
        # Use up to 2 existing negative test cases.
        for _, row in csv_negative.head(2).iterrows():
            final_neg_cases.append(format_csv_test_case(row))

        # ----------------------------------------------------------------
        # Step 2: Generate missing test cases via LLM if needed.
        # ----------------------------------------------------------------
        import re

        # For Positive Test Cases:
        if len(final_pos_cases) < 2:
            missing_count = 2 - len(final_pos_cases)
            pos_prompt = """
            [INST] Generate EXACTLY {missing_count} POSITIVE test case(s) for:
            Error: {error}
            Solution: {solution}
            
            Each test case MUST include the following sections, and end with the delimiter "### END TEST CASE ###":
            - **Test Scenario**: A short description of the scenario.
            - **Test Steps**: Step-by-step instructions.
            - **Pre Requisites**: Conditions before running the test.
            - **Expected Results**: What should happen if the solution works.
            - **Pass/Fail Criteria**: How to determine if the test passes.
            
            Output format (including the delimiter):
            **Test Scenario**: 
            **Test Steps**: 
            **Pre Requisites**: 
            **Expected Results**: 
            **Pass/Fail Criteria**: 
            ### END TEST CASE ###
            """.format(missing_count=missing_count, error=error_message, solution=solution)
            pos_template = ChatPromptTemplate.from_template(pos_prompt)
            formatted_pos = pos_template.format_prompt().to_messages()
            pos_response = llm.invoke(formatted_pos).content.strip()
            generated_pos = [tc.strip() for tc in re.split(r"\n### END TEST CASE ###\n", pos_response) if tc.strip()]
            # Append the generated ones (ensuring we only take the missing_count)
            final_pos_cases += generated_pos[:missing_count]

        # For Negative Test Cases:
        if len(final_neg_cases) < 2:
            missing_count = 2 - len(final_neg_cases)
            neg_prompt = """
            [INST] Generate EXACTLY {missing_count} NEGATIVE test case(s) for:
            Error: {error}
            Solution: {solution}
            
            Each test case MUST include the following sections, and end with the delimiter "### END TEST CASE ###":
            - **Test Scenario**: A short description of the scenario.
            - **Test Steps**: Step-by-step instructions.
            - **Pre Requisites**: Conditions before running the test.
            - **Expected Results**: What should happen if the solution fails.
            - **Pass/Fail Criteria**: How to determine if the test fails.
            
            Output format (including the delimiter):
            **Test Scenario**: 
            **Test Steps**: 
            **Pre Requisites**: 
            **Expected Results**: 
            **Pass/Fail Criteria**: 
            ### END TEST CASE ###
            """.format(missing_count=missing_count, error=error_message, solution=solution)
            neg_template = ChatPromptTemplate.from_template(neg_prompt)
            formatted_neg = neg_template.format_prompt().to_messages()
            neg_response = llm.invoke(formatted_neg).content.strip()
            generated_neg = [tc.strip() for tc in re.split(r"\n### END TEST CASE ###\n", neg_response) if tc.strip()]
            final_neg_cases += generated_neg[:missing_count]

        # ----------------------------------------------------------------
        # Step 3: Generate an explanation for why the solution fixes the error.
        # ----------------------------------------------------------------
        explanation_prompt = """
        [INST] Explain why this solution fixes the following error:
        Error: {error}
        Solution: {solution}
        [/INST]
        """
        explanation_template = ChatPromptTemplate.from_template(explanation_prompt)
        formatted_explanation = explanation_template.format_prompt(error=error_message, solution=solution)
        explanation = llm.invoke(formatted_explanation.to_messages()).content.strip()

        # ----------------------------------------------------------------
        # Step 4: Combine all parts into the final response.
        # ----------------------------------------------------------------
        test_cases_text = ""
        for idx, tc in enumerate(final_pos_cases, start=1):
            test_cases_text += f"**Positive Test Case {idx}:**\n{tc}\n\n"
        for idx, tc in enumerate(final_neg_cases, start=1):
            test_cases_text += f"**Negative Test Case {idx}:**\n{tc}\n\n"

        response_template = (
            "**Error:**\n{Error}\n\n"
            "**Solution:**\n{Solution}\n\n"
            "**Explanation:**\n{Explanation}\n\n"
            "**Final Test Cases (2 Positive and 2 Negative):**\n{TestCases}"
        )
        return {"response": response_template.format(
            Error=error_message,
            Solution=solution,
            Explanation=explanation,
            TestCases=test_cases_text
        )}

    except Exception as e:
        logging.error("Validation/Generation error: %s", str(e))
        return {"response": f"Error processing request: {str(e)}"}

# --------------------------------------------------------------------
# Build the State Graph Workflow
# --------------------------------------------------------------------
workflow = StateGraph(AgentState)
# "retrieve" node: fetch context using the input defect
workflow.add_node("retrieve", lambda state: {"context": retriever.invoke(state["input"])})
workflow.add_node("validate_or_generate_test_cases", validate_or_generate_test_cases)
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "validate_or_generate_test_cases")
workflow.add_edge("validate_or_generate_test_cases", END)
agent = workflow.compile()

# --------------------------------------------------------------------
# Automated Evaluation & Self-improvement Functions (unchanged)
# --------------------------------------------------------------------
def auto_evaluate_solution(response: str) -> int:
    # Simple heuristic: if response contains our delimiter, assume generated output is complete.
    if "### END TEST CASE ###" in response:
        return 5
    elif "**Error**:" in response:
        return 1
    else:
        return 3

def generate_alternative_solution(error_message: str) -> str:
    alt_prompt = """
    [INST] Provide a concise, actionable alternative solution for the following error:
    Error: {error}
    Ensure that the solution is clear and does not include any follow-up questions.
    [/INST]
    """
    alt_template = ChatPromptTemplate.from_template(alt_prompt)
    formatted_alt = alt_template.format_prompt(error=error_message)
    alternative_solution = llm.invoke(formatted_alt.to_messages()).content.strip()

    test_case_prompt = """
    [INST] Given the error and the alternative solution:
    Error: {error}
    Solution: {solution}
    Generate EXACTLY 4 structured test cases (2 positive and 2 negative) with the delimiter "### END TEST CASE ###" after each test case.
    Each test case must include:
      - **Test Scenario**
      - **Test Steps**
      - **Pre Requisites**
      - **Expected Results**
      - **Pass/Fail Criteria**
    [/INST]
    """
    tc_template = ChatPromptTemplate.from_template(test_case_prompt)
    formatted_tc = tc_template.format_prompt(error=error_message, solution=alternative_solution)
    alternative_test_cases = llm.invoke(formatted_tc.to_messages()).content.strip()

    alt_response = (
        "**Alternative Solution (Generated):**\n{AltSolution}\n\n"
        "**Test Cases for Alternative Solution:**\n{AltTestCases}"
    ).format(
        AltSolution=alternative_solution,
        AltTestCases=alternative_test_cases
    )
    return alt_response

def get_solution_autonomously(error_message: str) -> str:
    max_iterations = 3
    iteration = 0
    while iteration < max_iterations:
        logging.info("Iteration %d: Processing error: %s", iteration + 1, error_message)
        result = agent.invoke({"input": error_message.strip()})
        response = result["response"]
        logging.info("Agent response:\n%s", response)
        rating = auto_evaluate_solution(response)
        logging.info("Auto-evaluated rating: %d", rating)
        if rating < 3:
            logging.info("Rating below threshold. Generating alternative solution.")
            alt_response = generate_alternative_solution(error_message)
            logging.info("Alternative response generated.")
            return alt_response
        else:
            return response
        iteration += 1
    logging.info("Max iterations reached. Returning last response.")
    return response

# --------------------------------------------------------------------
# Autonomous Agent Execution
# --------------------------------------------------------------------
def main():
    error_description = "Search results not displaying correctly"
    final_solution = get_solution_autonomously(error_description)
    print("\n=== Final Autonomous Response ===\n")
    print(final_solution)

if __name__ == "__main__":
    main()
