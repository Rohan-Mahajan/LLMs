from google.colab import userdata
groq_api = userdata.get("groq_api_key")

import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langgraph.graph import END, StateGraph
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from typing import TypedDict, List, Optional

# Load and prepare data
df = pd.read_csv("/content/failure reason.csv")
docs = []
for _, row in df.iterrows():
    if pd.notna(row["Failure Reason"]) and pd.notna(row["Solution"]):
        docs.append(Document(
            page_content=row["Failure Reason"],
            metadata={"solution": row["Solution"]}
        ))

# Initialize embeddings and vector store
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_documents(docs, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 1})

# Define conversation state
class AgentState(TypedDict):
    user_input: str
    conversation_history: List[dict]
    current_context: Optional[dict]
    response: str

# Initialize LLM
llm = ChatGroq(
    groq_api_key=groq_api,
    temperature=0.3,
    model_name="llama3-8b-8192",
)

def manage_memory(state: AgentState):
    """Manage conversation history and context"""
    history = state.get("conversation_history", [])
    
    # Keep last 6 messages for context
    if len(history) > 6:
        history = history[-6:]
        
    return {"conversation_history": history}

def determine_intent(state: AgentState):
    """Determine if user is reporting a new error or discussing previous context"""
    history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["conversation_history"]])
    
    prompt = ChatPromptTemplate.from_template("""
    [INST] Analyze the conversation history and current input to determine user intent:

    Conversation History:
    {history}

    Current Input: {input}

    Is the user:
    1. Reporting a new system error (respond with 'error')
    2. Discussing previous test cases/solutions (respond with 'followup')
    3. General conversation/query (respond with 'general')

    Respond ONLY with one word: 'error', 'followup', or 'general' [/INST]
    """)
    
    intent = llm.invoke(prompt.format(history=history, input=state["user_input"])).content.strip().lower()
    return {"intent": intent}

def handle_new_error(state: AgentState):
    """Process new error reports"""
    try:
        relevant_docs = retriever.invoke(state["user_input"])
        if relevant_docs and "solution" in relevant_docs[0].metadata:
            return {
                "current_context": {
                    "error": state["user_input"],
                    "solution": relevant_docs[0].metadata["solution"],
                    "test_cases": None
                }
            }
        return {"current_context": None}
    except:
        return {"current_context": None}

def generate_test_cases(state: AgentState):
    """Generate initial test cases for a new error"""
    if not state["current_context"]:
        return {"response": "I couldn't find a solution for that error. Please provide more details."}
    
    prompt = ChatPromptTemplate.from_template("""
    [INST] Given this error and solution:
    Error: {error}
    Solution: {solution}

    Generate 4 test cases (2 positive, 2 negative) in this format:
    1. **[Positive/Negative]** [Scenario]
    - Steps: [numbered steps]
    - Expected: [expected outcome]
    - Criteria: [pass/fail criteria]

    Make them concise and technical. [/INST]
    """)
    
    test_cases = llm.invoke(prompt.format(
        error=state["current_context"]["error"],
        solution=state["current_context"]["solution"]
    )).content
    
    return {
        "current_context": {
            **state["current_context"],
            "test_cases": test_cases
        },
        "response": f"""**Solution Found**\n{state["current_context"]["solution"]}\n\n**Test Cases**\n{test_cases}"""
    }

def handle_followup(state: AgentState):
    """Handle discussions about existing test cases"""
    history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["conversation_history"]])
    context = state.get("current_context", {})
    
    prompt = ChatPromptTemplate.from_template("""
    [INST] You're a QA expert discussing these test cases:
    Error: {error}
    Solution: {solution}
    Test Cases: {test_cases}

    Conversation History:
    {history}

    User's latest message: {input}

    Provide a helpful technical response focusing on test case validation and troubleshooting.
    If asked about specific test cases, reference them by number (1-4).
    Keep responses under 150 words. [/INST]
    """)
    
    response = llm.invoke(prompt.format(
        error=context.get("error", "N/A"),
        solution=context.get("solution", "N/A"),
        test_cases=context.get("test_cases", "None generated"),
        history=history,
        input=state["user_input"]
    )).content
    
    return {"response": response}

def handle_general(state: AgentState):
    """Handle non-technical conversation"""
    prompt = ChatPromptTemplate.from_template("""
    [INST] You're a technical support assistant. Respond to this in a friendly, professional manner:
    {input}
    
    Keep response under 100 words. [/INST]
    """)
    
    return {"response": llm.invoke(prompt.format(input=state["user_input"])).content}

# Build workflow
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("manage_memory", manage_memory)
workflow.add_node("determine_intent", determine_intent)
workflow.add_node("handle_new_error", handle_new_error)
workflow.add_node("generate_test_cases", generate_test_cases)
workflow.add_node("handle_followup", handle_followup)
workflow.add_node("handle_general", handle_general)

# Set up edges
workflow.set_entry_point("manage_memory")
workflow.add_edge("manage_memory", "determine_intent")
workflow.add_edge("determine_intent", "handle_new_error", lambda s: s["intent"] == "error")
workflow.add_edge("handle_new_error", "generate_test_cases")
workflow.add_edge("generate_test_cases", END)

workflow.add_edge("determine_intent", "handle_followup", lambda s: s["intent"] == "followup")
workflow.add_edge("handle_followup", END)

workflow.add_edge("determine_intent", "handle_general", lambda s: s["intent"] == "general")
workflow.add_edge("handle_general", END)

# Compile agent
agent = workflow.compile()

# Chat interface
def chat_loop():
    conversation_history = []
    current_context = None
    
    print("QA Expert Bot: Hi! I'm your test case assistant. Describe an error or discuss existing cases.")
    
    while True:
        user_input = input("\nYou: ").strip()
        if user_input.lower() in ('quit', 'exit'):
            break
        
        # Run agent
        result = agent.invoke({
            "user_input": user_input,
            "conversation_history": conversation_history.copy(),
            "current_context": current_context
        })
        
        # Update state
        response = result.get("response", "I didn't quite get that. Could you rephrase?")
        current_context = result.get("current_context", current_context)
        
        # Store conversation
        conversation_history.append({"role": "user", "content": user_input})
        conversation_history.append({"role": "assistant", "content": response})
        
        print(f"\nBot: {response}")
    
    print("\nQA Expert Bot: Goodbye! Let me know if you need more help later.")

# Start chat
chat_loop()








import pandas as pd
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Annotated
import operator

# Load data files
defects_df = pd.read_csv("defect_dump.csv")
test_cases_df = pd.read_csv("amazon_test_cases.csv")

class AgentState(TypedDict):
    user_input: str
    defect_details: dict
    solution: str
    test_cases: List[dict]
    conversation_history: Annotated[List[str], operator.add]
    resolved: bool

def defect_resolution_agent(state: AgentState):
    user_input = state["user_input"].lower()
    
    # Search for defect in database
    defect_match = defects_df[
        defects_df["defect_description"].str.lower().str.contains(user_input)
    ]
    
    if not defect_match.empty:
        defect = defect_match.iloc[0].to_dict()
        return {
            "defect_details": defect,
            "solution": f"Solution: {defect['steps_taken_to_resolve']}\nRoot Cause: {defect['root_cause_analysis']}",
            "conversation_history": [f"Identified defect in {defect['module_name']}"]
        }
    
    return {"solution": "Defect not found in database. Please provide more details."}

def test_case_retrieval_agent(state: AgentState):
    if "defect_details" not in state:
        return {"test_cases": []}
    
    module_name = state["defect_details"]["module_name"]
    test_cases = test_cases_df[
        test_cases_df["test_description"].str.contains(module_name)
    ].to_dict("records")
    
    test_cases_formatted = "\n".join(
        [f"TC-{tc['test_case_id']}: {tc['test_description']}" for tc in test_cases]
    )
    
    return {
        "test_cases": test_cases,
        "conversation_history": [f"Found {len(test_cases)} relevant test cases"],
        "solution": f"{state['solution']}\n\nValidation Test Cases:\n{test_cases_formatted}"
    }

def validation_agent(state: AgentState):
    if not state["test_cases"]:
        return {"resolved": False}
    
    # Simulate validation check (would be actual validation in real system)
    expected_results = all(
        tc["expected_results"] in state["solution"]
        for tc in state["test_cases"]
    )
    
    return {
        "resolved": expected_results,
        "conversation_history": ["Validation completed"]
    }

def human_feedback(state: AgentState):
    response = input("Was the issue resolved? (y/n): ")
    return {"resolved": response.lower() == "y"}

def format_response(state: AgentState):
    response = f"{state['solution']}\n\n"
    if state["test_cases"]:
        response += "Validation Steps:\n"
        for tc in state["test_cases"]:
            response += f"{tc['test_case_id']}: {tc['test_steps']}\n"
    return response

# Build the agent workflow
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("defect_analysis", defect_resolution_agent)
workflow.add_node("retrieve_tests", test_case_retrieval_agent)
workflow.add_node("validate_solution", validation_agent)
workflow.add_node("get_human_feedback", human_feedback)
workflow.add_node("format_output", format_response)

# Set up edges
workflow.set_entry_point("defect_analysis")

workflow.add_edge("defect_analysis", "retrieve_tests")
workflow.add_edge("retrieve_tests", "validate_solution")
workflow.add_edge("validate_solution", "format_output")

# Conditional edges based on validation
workflow.add_conditional_edges(
    "format_output",
    lambda state: "resolved" if state.get("resolved") else "needs_feedback",
    {
        "resolved": END,
        "needs_feedback": "get_human_feedback"
    }
)

workflow.add_conditional_edges(
    "get_human_feedback",
    lambda state: "resolved" if state.get("resolved") else "defect_analysis",
    {
        "resolved": END,
        "defect_analysis": "defect_analysis"
    }
)

# Compile the workflow
app = workflow.compile()

# Chat interface
def chat():
    print("Defect Resolution Bot: Describe the system error you're encountering.")
    history = []
    state = {
        "user_input": "",
        "conversation_history": [],
        "resolved": False
    }
    
    while not state.get("resolved"):
        user_input = input("User: ")
        state["user_input"] = user_input
        state = app.invoke(state)
        
        print("\nDefect Resolution Bot:")
        print(state.get("solution", "Hmm, let me check that again..."))
        
        if "test_cases" in state and state["test_cases"]:
            print("\nValidation Test Cases:")
            for tc in state["test_cases"]:
                print(f"TC-{tc['test_case_id']}: {tc['test_steps']}")
        
    print("\nIssue resolved! Exiting chat.")

if __name__ == "__main__":
    chat()
