from google.colab import userdata
groq_api = userdata.get("groq_api_key")

import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langgraph.graph import END, StateGraph
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from typing import TypedDict, List, Optional

# Load and prepare data
df = pd.read_csv("/content/failure reason.csv")
docs = []
for _, row in df.iterrows():
    if pd.notna(row["Failure Reason"]) and pd.notna(row["Solution"]):
        docs.append(Document(
            page_content=row["Failure Reason"],
            metadata={"solution": row["Solution"]}
        ))

# Initialize embeddings and vector store
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_documents(docs, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 1})

# Define conversation state
class AgentState(TypedDict):
    user_input: str
    conversation_history: List[dict]
    current_context: Optional[dict]
    response: str

# Initialize LLM
llm = ChatGroq(
    groq_api_key=groq_api,
    temperature=0.3,
    model_name="llama3-8b-8192",
)

def manage_memory(state: AgentState):
    """Manage conversation history and context"""
    history = state.get("conversation_history", [])
    
    # Keep last 6 messages for context
    if len(history) > 6:
        history = history[-6:]
        
    return {"conversation_history": history}

def determine_intent(state: AgentState):
    """Determine if user is reporting a new error or discussing previous context"""
    history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["conversation_history"]])
    
    prompt = ChatPromptTemplate.from_template("""
    [INST] Analyze the conversation history and current input to determine user intent:

    Conversation History:
    {history}

    Current Input: {input}

    Is the user:
    1. Reporting a new system error (respond with 'error')
    2. Discussing previous test cases/solutions (respond with 'followup')
    3. General conversation/query (respond with 'general')

    Respond ONLY with one word: 'error', 'followup', or 'general' [/INST]
    """)
    
    intent = llm.invoke(prompt.format(history=history, input=state["user_input"])).content.strip().lower()
    return {"intent": intent}

def handle_new_error(state: AgentState):
    """Process new error reports"""
    try:
        relevant_docs = retriever.invoke(state["user_input"])
        if relevant_docs and "solution" in relevant_docs[0].metadata:
            return {
                "current_context": {
                    "error": state["user_input"],
                    "solution": relevant_docs[0].metadata["solution"],
                    "test_cases": None
                }
            }
        return {"current_context": None}
    except:
        return {"current_context": None}

def generate_test_cases(state: AgentState):
    """Generate initial test cases for a new error"""
    if not state["current_context"]:
        return {"response": "I couldn't find a solution for that error. Please provide more details."}
    
    prompt = ChatPromptTemplate.from_template("""
    [INST] Given this error and solution:
    Error: {error}
    Solution: {solution}

    Generate 4 test cases (2 positive, 2 negative) in this format:
    1. **[Positive/Negative]** [Scenario]
    - Steps: [numbered steps]
    - Expected: [expected outcome]
    - Criteria: [pass/fail criteria]

    Make them concise and technical. [/INST]
    """)
    
    test_cases = llm.invoke(prompt.format(
        error=state["current_context"]["error"],
        solution=state["current_context"]["solution"]
    )).content
    
    return {
        "current_context": {
            **state["current_context"],
            "test_cases": test_cases
        },
        "response": f"""**Solution Found**\n{state["current_context"]["solution"]}\n\n**Test Cases**\n{test_cases}"""
    }

def handle_followup(state: AgentState):
    """Handle discussions about existing test cases"""
    history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in state["conversation_history"]])
    context = state.get("current_context", {})
    
    prompt = ChatPromptTemplate.from_template("""
    [INST] You're a QA expert discussing these test cases:
    Error: {error}
    Solution: {solution}
    Test Cases: {test_cases}

    Conversation History:
    {history}

    User's latest message: {input}

    Provide a helpful technical response focusing on test case validation and troubleshooting.
    If asked about specific test cases, reference them by number (1-4).
    Keep responses under 150 words. [/INST]
    """)
    
    response = llm.invoke(prompt.format(
        error=context.get("error", "N/A"),
        solution=context.get("solution", "N/A"),
        test_cases=context.get("test_cases", "None generated"),
        history=history,
        input=state["user_input"]
    )).content
    
    return {"response": response}

def handle_general(state: AgentState):
    """Handle non-technical conversation"""
    prompt = ChatPromptTemplate.from_template("""
    [INST] You're a technical support assistant. Respond to this in a friendly, professional manner:
    {input}
    
    Keep response under 100 words. [/INST]
    """)
    
    return {"response": llm.invoke(prompt.format(input=state["user_input"])).content}

# Build workflow
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("manage_memory", manage_memory)
workflow.add_node("determine_intent", determine_intent)
workflow.add_node("handle_new_error", handle_new_error)
workflow.add_node("generate_test_cases", generate_test_cases)
workflow.add_node("handle_followup", handle_followup)
workflow.add_node("handle_general", handle_general)

# Set up edges
workflow.set_entry_point("manage_memory")
workflow.add_edge("manage_memory", "determine_intent")
workflow.add_edge("determine_intent", "handle_new_error", lambda s: s["intent"] == "error")
workflow.add_edge("handle_new_error", "generate_test_cases")
workflow.add_edge("generate_test_cases", END)

workflow.add_edge("determine_intent", "handle_followup", lambda s: s["intent"] == "followup")
workflow.add_edge("handle_followup", END)

workflow.add_edge("determine_intent", "handle_general", lambda s: s["intent"] == "general")
workflow.add_edge("handle_general", END)

# Compile agent
agent = workflow.compile()

# Chat interface
def chat_loop():
    conversation_history = []
    current_context = None
    
    print("QA Expert Bot: Hi! I'm your test case assistant. Describe an error or discuss existing cases.")
    
    while True:
        user_input = input("\nYou: ").strip()
        if user_input.lower() in ('quit', 'exit'):
            break
        
        # Run agent
        result = agent.invoke({
            "user_input": user_input,
            "conversation_history": conversation_history.copy(),
            "current_context": current_context
        })
        
        # Update state
        response = result.get("response", "I didn't quite get that. Could you rephrase?")
        current_context = result.get("current_context", current_context)
        
        # Store conversation
        conversation_history.append({"role": "user", "content": user_input})
        conversation_history.append({"role": "assistant", "content": response})
        
        print(f"\nBot: {response}")
    
    print("\nQA Expert Bot: Goodbye! Let me know if you need more help later.")

# Start chat
chat_loop()
