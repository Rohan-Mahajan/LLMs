from google.colab import userdata
groq_api = userdata.get("groq_api_key")

import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langgraph.graph import END, StateGraph
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from typing import TypedDict, List

# Load and prepare data
df = pd.read_csv("/content/failure reason.csv")
docs = []
for _, row in df.iterrows():
    if pd.notna(row["Failure Reason"]) and pd.notna(row["Solution"]):
        docs.append(Document(
            page_content=row["Failure Reason"],
            metadata={"solution": row["Solution"]}
        ))

# Initialize embeddings and vector store
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_documents(docs, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 1})

# Define state with memory
class AgentState(TypedDict):
    input: str
    context: List[dict]
    response: str
    history: List[str]  # Added conversation history

# Initialize LLM
llm = ChatGroq(
    groq_api_key=groq_api,
    temperature=0.3,
    model_name="llama3-8b-8192",
)

def retrieve(state: AgentState):
    try:
        relevant_docs = retriever.invoke(state["input"])
        return {"context": relevant_docs}
    except:
        return {"context": []}

def generate_response(state: AgentState):
    try:
        response_template = """**Error:**\n{Error}\n\n**Solution:**\n{Solution}\n\n**Test Cases:**\n{TestCases}"""

        if state["context"] and "solution" in state["context"][0].metadata:
            context = state["context"][0]
            # Get conversation history
            history_str = "\n".join(state["history"]) if state["history"] else "No previous conversation"
            
            prompt_template = """
            [INST] Consider our previous conversation:
            {history}
            
            Given this new error and its known solution:
            Error: {error}
            Solution: {solution}

            Generate **exactly** 4 structured test cases to validate that the solution fixes the issue:
            - 2 Positive Test Cases (where the solution works correctly)
            - 2 Negative Test Cases (where the solution fails or is misconfigured)

            Each test case should be structured in this format:
            1. **Test Case Scenario**: [Describe what is being tested]
            2. **Test Steps**: [Step-by-step actions to perform]
            3. **Expected Results**: [What should happen if the solution is correct]
            4. **Pass/Fail Criteria**: [How to determine if the test passes or fails]

            Maintain consistency with previous test cases if this is a follow-up question.
            [/INST]
            """
            result = llm.invoke(ChatPromptTemplate.from_template(prompt_template).format(
                history=history_str,
                error=state["input"],
                solution=context.metadata["solution"]
            )).content
            
            return {"response": response_template.format(
                Error=state["input"],
                Solution=context.metadata["solution"],
                TestCases=result.strip()
            )}
        else:
            return {"response": "**Error:** The error message is unknown, and test cases cannot be generated at this time."}

    except Exception as e:
        return {"response": f"Error processing request: {str(e)}"}

# Setup workflow
workflow = StateGraph(AgentState)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate_response", generate_response)
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate_response")
workflow.add_edge("generate_response", END)
agent = workflow.compile()

# Chat interface with memory
def chat_loop():
    history = []
    print("Welcome to the Failure Solution Expert!\nType 'quit' or 'exit' to end the conversation.")
    
    while True:
        user_input = input("\nUser: ").strip()
        if user_input.lower() in ('quit', 'exit'):
            break
            
        # Run agent with current state and history
        result = agent.invoke({
            "input": user_input,
            "history": history.copy()
        })
        
        response = result["response"]
        print(f"\nAssistant: {response}")
        
        # Update conversation history
        history.append(f"User: {user_input}")
        history.append(f"Assistant: {response}")
        
        # Keep last 4 exchanges to prevent context overflow
        history = history[-8:]

    print("\nGoodbye! Have a great day!")

# Start the chat
chat_loop()
