from typing import TypedDict, List, Literal
from langgraph.graph import END, StateGraph

# Modified Agent State with memory
class AgentState(TypedDict):
    input: str
    context: List[dict]
    response: str
    revision_count: int  # Track iteration attempts
    status: Literal["pending", "needs_revision", "approved", "failed"]

# New quality check function
def quality_evaluator(state: AgentState):
    response = state["response"]
    quality_issues = []
    
    # Check for required components
    if "Positive Test Cases" not in response:
        quality_issues.append("Missing positive test cases")
    if "Negative Test Cases" not in response:
        quality_issues.append("Missing negative test cases")
        
    # Check test case structure
    test_case_count = response.count("**Test Case Scenario**:")
    if test_case_count != 4:
        quality_issues.append(f"Found {test_case_count}/4 test cases")
    
    if quality_issues:
        return {
            "status": "needs_revision",
            "feedback": "Quality issues detected:\n- " + "\n- ".join(quality_issues)
        }
    return {"status": "approved"}

# Revised response generator with feedback incorporation
def generate_response(state: AgentState):
    try:
        # Initialize test cases if first attempt
        if state["revision_count"] == 0:
            if not state.get("context"):
                return {
                    "response": "Error: No known solution found.",
                    "status": "failed"
                }
            
            context = state["context"][0]
            prompt = """..."""  # Keep your original prompt
        else:
            # Add feedback to the prompt for revisions
            prompt = f"""REVISION REQUEST ({state['revision_count']}):
            {state.get('feedback', '')}
            
            Original Problem:
            Error: {state["input"]}
            Solution: {context.metadata["solution"]}
            
            Previous Attempt:
            {state["response"]}
            
            Please improve the test cases addressing the feedback above."""
        
        # Generate/regenerate response
        result = llm.invoke(ChatPromptTemplate.from_template(prompt)).content
        
        return {
            "response": result,
            "revision_count": state["revision_count"] + 1,
            "status": "pending"
        }
    except Exception as e:
        return {"response": str(e), "status": "failed"}

# New workflow with feedback loop
workflow = StateGraph(AgentState)

# Define nodes
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate_response)
workflow.add_node("evaluate", quality_evaluator)

# Conditional edges
workflow.add_conditional_edges(
    "evaluate",
    lambda state: state["status"],
    {
        "approved": END,
        "needs_revision": "generate",
        "failed": END
    }
)

# Set up flow with safeguards
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", "evaluate")

# Add loopback with max retry check
workflow.add_conditional_edges(
    "generate",
    lambda state: "failed" if state["revision_count"] >= 3 else "continue",
    {"continue": "evaluate", "failed": END}
)

agent = workflow.compile()

# Modified invocation handler
def get_solution(error_message):
    state = {
        "input": error_message.strip(),
        "revision_count": 0,
        "status": "pending",
        "context": [],
        "response": ""
    }
    
    for _ in range(4):  # Max 3 revisions + initial attempt
        state = agent.invoke(state)
        if state["status"] in ["approved", "failed"]:
            break
            
    if state["status"] == "approved":
        return state["response"]
    elif state["revision_count"] >= 3:
        return "Failed to generate valid test cases after 3 attempts"
    return state["response"]

# Test it
print("\n=== Agentic Test ===")
print(get_solution("API rate limiting not enforced"))










from google.colab import userdata
groq_api = userdata.get("groq_api_key")

import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langgraph.graph import END, StateGraph
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from typing import TypedDict, List, Dict

# ------------------------------------------------------------------
# 1. Load CSV data and prepare Documents
# ------------------------------------------------------------------
df = pd.read_csv("/content/failure reason.csv")
docs = []
for _, row in df.iterrows():
    if pd.notna(row["Failure Reason"]) and pd.notna(row["Solution"]):
        docs.append(Document(
            page_content=row["Failure Reason"],
            metadata={"solution": row["Solution"]}
        ))

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_documents(docs, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 1})

# ------------------------------------------------------------------
# 2. Extend agent state with conversation memory
# ------------------------------------------------------------------
class AgentState(TypedDict):
    input: str
    context: List[dict]
    response: str
    history: List[Dict[str, str]]  # Each history entry: {"user": ..., "bot": ...}

# ------------------------------------------------------------------
# 3. Initialize the LLM
# ------------------------------------------------------------------
llm = ChatGroq(
    groq_api_key=groq_api,
    temperature=0.3,  # Slightly higher temperature for creative generation
    model_name="gemma2-9b-it",
)

# ------------------------------------------------------------------
# 4. Define helper functions
# ------------------------------------------------------------------
def retrieve(state: AgentState) -> Dict[str, List[dict]]:
    """
    Optionally incorporate previous conversation history by concatenating prior turns.
    """
    # If history exists, join previous user and bot messages to give context.
    history_text = ""
    if state["history"]:
        history_text = " ".join([f"User: {entry['user']} Bot: {entry['bot']}" for entry in state["history"]])
    # Combine history with the current error message.
    input_with_history = f"{history_text} {state['input']}".strip()
    try:
        relevant_docs = retriever.invoke(input_with_history)
        return {"context": relevant_docs} if relevant_docs else {"context": []}
    except Exception as e:
        return {"context": []}

def generate_response(state: AgentState) -> Dict[str, str]:
    """
    Generate a response using the retrieved solution.
    Then, call the LLM to generate exactly 4 structured test cases.
    """
    try:
        response_template = """**Error:**\n{Error}\n\n**Solution:**\n{Solution}\n\n**Test Cases:**\n{TestCases}"""
        if state["context"] and "solution" in state["context"][0].metadata:
            context_doc = state["context"][0]
            prompt_template = """
[INST] Given this error and known solution:
Error: {error}
Solution: {solution}

Generate **exactly** 4 structured test cases to validate that the solution fixes the issue:
- 2 Positive Test Cases (where the solution works correctly)
- 2 Negative Test Cases (where the solution fails or is misconfigured)

Each test case should be unique and structured in this format:
1 **Test Scenario**: Describe what is being tested.
2 **Test Steps**: Step-by-step actions to perform.
3 **Expected Results**: What should happen if the solution is correct.
4 **Pass/Fail Criteria**: How to determine if the test passes or fails.
[/INST]
            """
            formatted_prompt = ChatPromptTemplate.from_template(prompt_template).format(
                error=state["input"],
                solution=context_doc.metadata["solution"]
            )
            # Invoke the LLM to generate test cases
            result = llm.invoke(formatted_prompt).content
            return {"response": response_template.format(
                Error=state["input"],
                Solution=context_doc.metadata["solution"],
                TestCases=result.strip()
            )}
        else:
            return {"response": "**Error:** The error message is unknown and cannot be resolved."}
    except Exception as e:
        return {"response": f"Error processing request: {str(e)}"}

# ------------------------------------------------------------------
# 5. Build the workflow and compile the agent
# ------------------------------------------------------------------
workflow = StateGraph(AgentState)
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate_response", generate_response)
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate_response")
workflow.add_edge("generate_response", END)
agent = workflow.compile()

def get_solution(error_message: str, history: List[Dict[str, str]]) -> str:
    state: AgentState = {
        "input": error_message.strip(),
        "context": [],
        "response": "",
        "history": history
    }
    result = agent.invoke(state)
    return result["response"]

# ------------------------------------------------------------------
# 6. Define ChatAgent class with interactive loop and memory
# ------------------------------------------------------------------
class ChatAgent:
    def __init__(self):
        # Initialize conversation history as an empty list.
        self.history: List[Dict[str, str]] = []

    def process_input(self, user_input: str) -> str:
        """
        Call get_solution using the current input and history, then update the history.
        """
        solution = get_solution(user_input, self.history)
        # Update conversation history with this turn.
        self.history.append({"user": user_input, "bot": solution})
        return solution

    def interactive_chat(self):
        print("Welcome to the Agentic Error Resolution Chatbot!")
        print("Type your error description (or type 'quit' to exit).\n")
        while True:
            user_input = input("You: ").strip()
            if user_input.lower() == "quit":
                print("Goodbye!")
                break
            # Get bot's response for the error message.
            solution = self.process_input(user_input)
            print("\n--- Proposed Solution ---")
            print(solution)
            print("-------------------------\n")
            # Ask for feedback from the user.
            feedback = input("Was this solution helpful? (yes/no): ").strip().lower()
            if feedback.startswith("n"):
                additional_info = input("Please provide additional details or clarification about the error: ").strip()
                # Combine the original input with additional details and re-run.
                refined_input = f"{user_input} {additional_info}"
                refined_solution = self.process_input(refined_input)
                print("\n--- Refined Solution ---")
                print(refined_solution)
                print("-------------------------\n")
            # (Optional) You could decide to clear history if the error is resolved.
            # For now, we keep accumulating history for context.

# ------------------------------------------------------------------
# 7. Start an interactive chat session
# ------------------------------------------------------------------
if __name__ == "__main__":
    chat_agent = ChatAgent()
    chat_agent.interactive_chat()

