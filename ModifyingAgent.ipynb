from typing import TypedDict, List, Literal
from langgraph.graph import END, StateGraph

# Modified Agent State with memory
class AgentState(TypedDict):
    input: str
    context: List[dict]
    response: str
    revision_count: int  # Track iteration attempts
    status: Literal["pending", "needs_revision", "approved", "failed"]

# New quality check function
def quality_evaluator(state: AgentState):
    response = state["response"]
    quality_issues = []
    
    # Check for required components
    if "Positive Test Cases" not in response:
        quality_issues.append("Missing positive test cases")
    if "Negative Test Cases" not in response:
        quality_issues.append("Missing negative test cases")
        
    # Check test case structure
    test_case_count = response.count("**Test Case Scenario**:")
    if test_case_count != 4:
        quality_issues.append(f"Found {test_case_count}/4 test cases")
    
    if quality_issues:
        return {
            "status": "needs_revision",
            "feedback": "Quality issues detected:\n- " + "\n- ".join(quality_issues)
        }
    return {"status": "approved"}

# Revised response generator with feedback incorporation
def generate_response(state: AgentState):
    try:
        # Initialize test cases if first attempt
        if state["revision_count"] == 0:
            if not state.get("context"):
                return {
                    "response": "Error: No known solution found.",
                    "status": "failed"
                }
            
            context = state["context"][0]
            prompt = """..."""  # Keep your original prompt
        else:
            # Add feedback to the prompt for revisions
            prompt = f"""REVISION REQUEST ({state['revision_count']}):
            {state.get('feedback', '')}
            
            Original Problem:
            Error: {state["input"]}
            Solution: {context.metadata["solution"]}
            
            Previous Attempt:
            {state["response"]}
            
            Please improve the test cases addressing the feedback above."""
        
        # Generate/regenerate response
        result = llm.invoke(ChatPromptTemplate.from_template(prompt)).content
        
        return {
            "response": result,
            "revision_count": state["revision_count"] + 1,
            "status": "pending"
        }
    except Exception as e:
        return {"response": str(e), "status": "failed"}

# New workflow with feedback loop
workflow = StateGraph(AgentState)

# Define nodes
workflow.add_node("retrieve", retrieve)
workflow.add_node("generate", generate_response)
workflow.add_node("evaluate", quality_evaluator)

# Conditional edges
workflow.add_conditional_edges(
    "evaluate",
    lambda state: state["status"],
    {
        "approved": END,
        "needs_revision": "generate",
        "failed": END
    }
)

# Set up flow with safeguards
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", "evaluate")

# Add loopback with max retry check
workflow.add_conditional_edges(
    "generate",
    lambda state: "failed" if state["revision_count"] >= 3 else "continue",
    {"continue": "evaluate", "failed": END}
)

agent = workflow.compile()

# Modified invocation handler
def get_solution(error_message):
    state = {
        "input": error_message.strip(),
        "revision_count": 0,
        "status": "pending",
        "context": [],
        "response": ""
    }
    
    for _ in range(4):  # Max 3 revisions + initial attempt
        state = agent.invoke(state)
        if state["status"] in ["approved", "failed"]:
            break
            
    if state["status"] == "approved":
        return state["response"]
    elif state["revision_count"] >= 3:
        return "Failed to generate valid test cases after 3 attempts"
    return state["response"]

# Test it
print("\n=== Agentic Test ===")
print(get_solution("API rate limiting not enforced"))
